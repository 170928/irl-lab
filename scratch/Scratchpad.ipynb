{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "<ul>\n",
    "<li> Currently, we are dealing with only deterministic policies. Have to extend the implementation to stochastic policies. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self,GRID_SIZE=50):\n",
    "        self.grid_size = GRID_SIZE\n",
    "        self.num_states = self.grid_size**2\n",
    "        self.rewards = np.random.choice([0,-1],p=[0.67,0.33],size=(self.grid_size,self.grid_size))\n",
    "        self.rewards[0,self.grid_size-1] = 1\n",
    "        self.goal_state = self.grid_size-1\n",
    "        self.actions = np.array([\"up\",\"down\",\"left\",\"right\"])\n",
    "        \n",
    "    def get_feature(self,state):\n",
    "        _ = [0]*self.num_states\n",
    "        _[state] = 1\n",
    "        return _\n",
    "        \n",
    "    \n",
    "    def get_rewards(self):\n",
    "        return self.rewards.flatten()\n",
    "    \n",
    "    def result_of_action(self,state,action):\n",
    "        state_coords = (state/self.grid_size,state%self.grid_size)\n",
    "        next_states = [(max(0,state_coords[0]-1),state_coords[1]),(min(self.grid_size-1,state_coords[0]+1),state_coords[1]),\\\n",
    "                      (state_coords[0],max(0,state_coords[1]-1)),(state_coords[0],min(self.grid_size-1,state_coords[1]+1))]\n",
    "        transition_probs = 0.1*np.ones((len(self.actions)))\n",
    "        transition_probs[np.where(self.actions == action)[0][0]] = 0.7\n",
    "        next_state = next_states[np.random.choice(range(len(next_states)),p=transition_probs)]\n",
    "        return next_state[0]*self.grid_size+next_state[1]\n",
    "    \n",
    "    def generate_trajectory(self,policy=None,num_trajectories=10):\n",
    "        if policy is None:\n",
    "            policy = np.random.choice(self.actions,size=(self.num_states))\n",
    "        trajectories = []\n",
    "        for i in range(num_trajectories):\n",
    "            trajectory = []\n",
    "            current_state = np.random.randint(self.num_states)\n",
    "            while current_state != self.goal_state and len(trajectory) < self.grid_size*3:\n",
    "                trajectory.append(self.get_feature(current_state))\n",
    "                current_state = self.result_of_action(current_state,policy[current_state])\n",
    "            if current_state == self.goal_state:\n",
    "                trajectory.append(self.get_feature(self.goal_state))\n",
    "            trajectories.append(np.array(trajectory))\n",
    "        return np.array(trajectories)\n",
    "        \n",
    "    \n",
    "    def get_transition_probabilities(self,state,action):\n",
    "        '''While calculating the transition probabilities, we make the assumption that if you were in a cell along\n",
    "        the border, and you tried to make a transition outside the border with probability p, you end up not\n",
    "        moving with the same probability p.'''\n",
    "        transition_probs = np.zeros((self.grid_size,self.grid_size))\n",
    "        state_coords = (state/self.grid_size,state%self.grid_size)\n",
    "        if action == \"up\":\n",
    "            transition_probs[max(0,state_coords[0]-1),state_coords[1]] += 0.7 # up\n",
    "            transition_probs[state_coords[0],max(0,state_coords[1]-1)] += 0.1 # left\n",
    "            transition_probs[min(self.grid_size-1,state_coords[0]+1),state_coords[1]] += 0.1 # down\n",
    "            transition_probs[state_coords[0],min(self.grid_size-1,state_coords[1]+1)] += 0.1 #right\n",
    "        elif action == \"down\":\n",
    "            transition_probs[max(0,state_coords[0]-1),state_coords[1]] += 0.1\n",
    "            transition_probs[state_coords[0],max(0,state_coords[1]-1)] += 0.1\n",
    "            transition_probs[min(self.grid_size-1,state_coords[0]+1),state_coords[1]] += 0.7\n",
    "            transition_probs[state_coords[0],min(self.grid_size-1,state_coords[1]+1)] += 0.1\n",
    "        elif action == \"left\":\n",
    "            transition_probs[max(0,state_coords[0]-1),state_coords[1]] += 0.1\n",
    "            transition_probs[state_coords[0],max(0,state_coords[1]-1)] += 0.7\n",
    "            transition_probs[min(self.grid_size-1,state_coords[0]+1),state_coords[1]] += 0.1\n",
    "            transition_probs[state_coords[0],min(self.grid_size-1,state_coords[1]+1)] += 0.1\n",
    "        elif action == \"right\":\n",
    "            transition_probs[max(0,state_coords[0]-1),state_coords[1]] += 0.1\n",
    "            transition_probs[state_coords[0],max(0,state_coords[1]-1)] += 0.1\n",
    "            transition_probs[min(self.grid_size-1,state_coords[0]+1),state_coords[1]] += 0.1\n",
    "            transition_probs[state_coords[0],min(self.grid_size-1,state_coords[1]+1)] += 0.7\n",
    "        return transition_probs.flatten()\n",
    "    \n",
    "    def take_greedy_action(self,values):\n",
    "        values = values.reshape(self.grid_size,self.grid_size)\n",
    "        policy = np.repeat(\"random\",self.num_states)\n",
    "        for i in range(self.num_states):\n",
    "            state_coords = (i/self.grid_size,i%self.grid_size)\n",
    "            policy[i] = self.actions[np.argmax([values[max(0,state_coords[0]-1),state_coords[1]],\n",
    "                                                values[min(self.grid_size-1,state_coords[0]+1),state_coords[1]],\n",
    "                                                values[state_coords[0],max(0,state_coords[1]-1)],\n",
    "                                                values[state_coords[0],min(self.grid_size-1,state_coords[1]+1)]])]\n",
    "        return policy\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1  0  0 -1  0 -1 -1 -1 -1  1]\n",
      " [ 0  0  0  0 -1  0 -1  0  0 -1]\n",
      " [ 0 -1 -1  0  0 -1  0 -1  0  0]\n",
      " [ 0 -1 -1  0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1 -1  0 -1]\n",
      " [ 0  0  0 -1  0  0  0 -1  0 -1]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0 -1  0 -1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1 -1  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "class PolicyIteration:\n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        self.values = np.zeros((self.env.num_states,))\n",
    "        self.policy = np.random.choice(self.env.actions,size=(self.env.num_states))\n",
    "    \n",
    "    def policy_evaluation(self,num_iters=10,gamma=0.99):\n",
    "        for i in range(num_iters):\n",
    "            transition_probs = np.zeros((self.env.num_states,self.env.num_states))\n",
    "            for j in range(self.env.num_states):\n",
    "                transition_probs[j] = self.env.get_transition_probabilities(j,self.policy[j])\n",
    "            self.values = self.env.get_rewards() + gamma*np.dot(transition_probs,self.values)\n",
    "    \n",
    "    def policy_iteration(self,num_iters=10):\n",
    "        for i in range(num_iters):\n",
    "            self.policy_evaluation()\n",
    "            self.policy = self.env.take_greedy_action(self.values)\n",
    "        return self.policy\n",
    "        \n",
    "\n",
    "gw = GridWorld(10)\n",
    "print gw.rewards\n",
    "pi = PolicyIteration(gw)\n",
    "optimal_policy = pi.policy_iteration(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.12622733 -0.10404033  0.          0.         -0.22384805 -0.46498198\n",
      "  -0.21502754 -0.74728053  0.09801     0.87161908]\n",
      " [-0.86868083 -0.16352218 -0.16188696 -0.16026809 -0.15866541 -0.15262418\n",
      "   0.09227447 -0.30311082  0.39517875  1.29766903]\n",
      " [-0.81787899 -0.23538422  0.          0.          0.          0.          0.1\n",
      "  -0.07586245  0.33762384  0.25980982]\n",
      " [ 0.         -0.4831602   0.          0.          0.45344428  0.82225253\n",
      "   0.27893693 -0.45360797 -0.39070429  0.        ]\n",
      " [ 0.         -0.67250064  0.09813165  0.28936172  0.72937569  0.56270725\n",
      "  -0.18012098 -0.84448055 -0.87283156  0.        ]\n",
      " [-0.28168265 -0.07602425 -0.42437219 -0.43680115  0.64427006  0.194099\n",
      "  -0.09727599 -0.61002607 -0.28081265 -0.21165468]\n",
      " [-0.55766636 -0.44222197 -0.68270406 -0.56360288  0.13821161  0.          0.\n",
      "  -0.44946401 -0.19821716 -0.16545535]\n",
      " [-0.10217138 -0.3769091  -0.57448197  0.04985926 -0.04963934  0.          0.\n",
      "  -0.22796876 -0.69380162 -0.78261119]\n",
      " [-0.87916683 -0.41805071 -0.4166595  -0.01014514  0.          0.          0.\n",
      "  -0.14442522 -0.38663269 -0.09769162]\n",
      " [-0.05818257 -0.45987178  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "def feature_averages(trajectory,gamma=0.99):\n",
    "    horizon = len(trajectory)\n",
    "    return np.sum(np.multiply(trajectory,np.array([gamma**j for j in range(horizon)]).reshape(horizon,1)),axis=0)\n",
    "\n",
    "class RelEntIRL:\n",
    "    def __init__(self,expert_demos,nonoptimal_demos):\n",
    "        self.expert_demos = expert_demos\n",
    "        self.nonoptimal_demos = nonoptimal_demos\n",
    "        self.num_features = len(self.expert_demos[0][0])\n",
    "        self.weights = np.zeros((self.num_features,))\n",
    "    \n",
    "    def calculate_expert_feature(self):\n",
    "        self.expert_feature = np.zeros_like(self.weights)\n",
    "        for i in range(len(self.expert_demos)):\n",
    "            self.expert_feature += feature_averages(self.expert_demos[i])\n",
    "        self.expert_feature /= len(self.expert_demos)\n",
    "        return self.expert_feature\n",
    "    \n",
    "    def train(self,num_iters=10000,step_size=1e-4):\n",
    "        self.calculate_expert_feature()\n",
    "        self.policy_features = np.zeros((len(self.nonoptimal_demos),self.num_features))\n",
    "        for i in range(len(self.nonoptimal_demos)):\n",
    "            self.policy_features[i] = feature_averages(self.nonoptimal_demos[i])\n",
    "            \n",
    "        importance_sampling = np.zeros((len(self.nonoptimal_demos),))\n",
    "        for i in range(num_iters):\n",
    "            update = np.zeros_like(self.weights)\n",
    "            for j in range(len(self.nonoptimal_demos)):\n",
    "                importance_sampling[j] = np.exp(np.dot(self.policy_features[j],self.weights))\n",
    "            importance_sampling /= np.sum(importance_sampling,axis=0)\n",
    "            weighted_sum = np.sum(np.multiply(np.array([importance_sampling,]*self.policy_features.shape[1]).T,\\\n",
    "                                              self.policy_features),axis=0)\n",
    "            self.weights += step_size*(self.expert_feature - weighted_sum)\n",
    "                \n",
    "    \n",
    "expert_trajectories = gw.generate_trajectory(optimal_policy)\n",
    "nonoptimal_trajectories = gw.generate_trajectory()\n",
    "relent = RelEntIRL(expert_trajectories,nonoptimal_trajectories)\n",
    "relent.train()\n",
    "print relent.weights.reshape(10,10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
