{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "<ul>\n",
    "<li> Currently, we are dealing with only deterministic policies. Have to extend the implementation to stochastic policies. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self,GRID_SIZE=50):\n",
    "        self.grid_size = GRID_SIZE\n",
    "        self.num_states = self.grid_size**2\n",
    "        self.rewards = np.random.choice([0,-1],p=[0.67,0.33],size=(self.grid_size,self.grid_size))\n",
    "        self.rewards[0,self.grid_size-1] = 1\n",
    "        self.goal_state = self.grid_size-1\n",
    "        self.actions = np.array([\"up\",\"down\",\"left\",\"right\"])\n",
    "        \n",
    "    def get_feature(self,state):\n",
    "        _ = [0]*self.num_states\n",
    "        _[state] = 1\n",
    "        return _\n",
    "        \n",
    "    def get_rewards(self):\n",
    "        return self.rewards.flatten()\n",
    "    \n",
    "    def result_of_action(self,state,action):\n",
    "        state_coords = (state/self.grid_size,state%self.grid_size)\n",
    "        next_states = [(max(0,state_coords[0]-1),state_coords[1]),(min(self.grid_size-1,state_coords[0]+1),state_coords[1]),\\\n",
    "                      (state_coords[0],max(0,state_coords[1]-1)),(state_coords[0],min(self.grid_size-1,state_coords[1]+1))]\n",
    "        transition_probs = 0.1*np.ones((len(self.actions)))\n",
    "        transition_probs[np.where(self.actions == action)[0][0]] = 0.7\n",
    "        next_state = next_states[np.random.choice(range(len(next_states)),p=transition_probs)]\n",
    "        return next_state[0]*self.grid_size+next_state[1]\n",
    "    \n",
    "    def generate_trajectory(self,policy=None,num_trajectories=10):\n",
    "        if policy is None:\n",
    "            policy = np.random.choice(self.actions,size=(self.num_states))\n",
    "        trajectories = []\n",
    "        for i in range(num_trajectories):\n",
    "            trajectory = []\n",
    "            current_state = np.random.randint(self.num_states)\n",
    "            while current_state != self.goal_state and len(trajectory) < self.grid_size*3:\n",
    "                trajectory.append(self.get_feature(current_state))\n",
    "                current_state = self.result_of_action(current_state,policy[current_state])\n",
    "            if current_state == self.goal_state:\n",
    "                trajectory.append(self.get_feature(self.goal_state))\n",
    "            trajectories.append(np.array(trajectory))\n",
    "        return np.array(trajectories)\n",
    "        \n",
    "    \n",
    "    def get_transition_probabilities(self,state,action):\n",
    "        '''While calculating the transition probabilities, we make the assumption that if you were in a cell along\n",
    "        the border, and you tried to make a transition outside the border with probability p, you end up not\n",
    "        moving with the same probability p.'''\n",
    "        transition_probs = np.zeros((self.grid_size,self.grid_size))\n",
    "        state_coords = (state/self.grid_size,state%self.grid_size)\n",
    "        if action == \"up\":\n",
    "            transition_probs[max(0,state_coords[0]-1),state_coords[1]] += 0.7 # up\n",
    "            transition_probs[state_coords[0],max(0,state_coords[1]-1)] += 0.1 # left\n",
    "            transition_probs[min(self.grid_size-1,state_coords[0]+1),state_coords[1]] += 0.1 # down\n",
    "            transition_probs[state_coords[0],min(self.grid_size-1,state_coords[1]+1)] += 0.1 #right\n",
    "        elif action == \"down\":\n",
    "            transition_probs[max(0,state_coords[0]-1),state_coords[1]] += 0.1\n",
    "            transition_probs[state_coords[0],max(0,state_coords[1]-1)] += 0.1\n",
    "            transition_probs[min(self.grid_size-1,state_coords[0]+1),state_coords[1]] += 0.7\n",
    "            transition_probs[state_coords[0],min(self.grid_size-1,state_coords[1]+1)] += 0.1\n",
    "        elif action == \"left\":\n",
    "            transition_probs[max(0,state_coords[0]-1),state_coords[1]] += 0.1\n",
    "            transition_probs[state_coords[0],max(0,state_coords[1]-1)] += 0.7\n",
    "            transition_probs[min(self.grid_size-1,state_coords[0]+1),state_coords[1]] += 0.1\n",
    "            transition_probs[state_coords[0],min(self.grid_size-1,state_coords[1]+1)] += 0.1\n",
    "        elif action == \"right\":\n",
    "            transition_probs[max(0,state_coords[0]-1),state_coords[1]] += 0.1\n",
    "            transition_probs[state_coords[0],max(0,state_coords[1]-1)] += 0.1\n",
    "            transition_probs[min(self.grid_size-1,state_coords[0]+1),state_coords[1]] += 0.1\n",
    "            transition_probs[state_coords[0],min(self.grid_size-1,state_coords[1]+1)] += 0.7\n",
    "        return transition_probs.flatten()\n",
    "    \n",
    "    def take_greedy_action(self,values):\n",
    "        values = values.reshape(self.grid_size,self.grid_size)\n",
    "        policy = np.repeat(\"random\",self.num_states)\n",
    "        for i in range(self.num_states):\n",
    "            state_coords = (i/self.grid_size,i%self.grid_size)\n",
    "            policy[i] = self.actions[np.argmax([values[max(0,state_coords[0]-1),state_coords[1]],\n",
    "                                                values[min(self.grid_size-1,state_coords[0]+1),state_coords[1]],\n",
    "                                                values[state_coords[0],max(0,state_coords[1]-1)],\n",
    "                                                values[state_coords[0],min(self.grid_size-1,state_coords[1]+1)]])]\n",
    "        return policy\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0 -1 -1  0  1]\n",
      " [-1  0 -1 -1  0  0 -1  0  0  0]\n",
      " [ 0  0 -1 -1  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -1 -1  0 -1  0  0]\n",
      " [-1  0  0 -1  0  0 -1  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 -1 -1]\n",
      " [ 0  0  0  0  0  0 -1 -1  0 -1]\n",
      " [ 0  0 -1  0 -1  0 -1 -1  0  0]]\n"
     ]
    }
   ],
   "source": [
    "class PolicyIteration:\n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        self.values = np.zeros((self.env.num_states,))\n",
    "        self.policy = np.random.choice(self.env.actions,size=(self.env.num_states))\n",
    "    \n",
    "    def policy_evaluation(self,num_iters=10,gamma=0.99):\n",
    "        for i in range(num_iters):\n",
    "            transition_probs = np.zeros((self.env.num_states,self.env.num_states))\n",
    "            for j in range(self.env.num_states):\n",
    "                transition_probs[j] = self.env.get_transition_probabilities(j,self.policy[j])\n",
    "            self.values = self.env.get_rewards() + gamma*np.dot(transition_probs,self.values)\n",
    "    \n",
    "    def policy_iteration(self,num_iters=10):\n",
    "        for i in range(num_iters):\n",
    "            self.policy_evaluation()\n",
    "            self.policy = self.env.take_greedy_action(self.values)\n",
    "        return self.policy\n",
    "        \n",
    "\n",
    "gw = GridWorld(10)\n",
    "print gw.rewards\n",
    "pi = PolicyIteration(gw)\n",
    "optimal_policy = pi.policy_iteration(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of objective is: -0.142544491081\n",
      "Value of objective is: 0.462285878028\n",
      "Value of objective is: 0.463308118306\n",
      "Value of objective is: 0.463393935423\n",
      "Value of objective is: 0.463401887756\n",
      "Value of objective is: 0.463402635011\n",
      "Value of objective is: 0.463402705498\n",
      "Value of objective is: 0.463402712156\n",
      "Value of objective is: 0.463402712785\n",
      "Value of objective is: 0.463402712844\n",
      "[[-0.13 -0.03  0.06  0.06  0.06  0.08  0.06  0.04  0.08  0.19]\n",
      " [-0.03 -0.03  0.04  0.02  0.    0.02  0.02  0.04  0.14  0.12]\n",
      " [-0.03 -0.2  -0.04  0.02  0.    0.    0.    0.06  0.14  0.13]\n",
      " [-0.05 -0.28 -0.04 -0.04 -0.04 -0.04  0.    0.02  0.12  0.09]\n",
      " [-0.02 -0.19 -0.05  0.02 -0.01 -0.09 -0.06  0.01  0.09  0.04]\n",
      " [-0.12 -0.14 -0.06 -0.17 -0.09 -0.01  0.01  0.08  0.07  0.04]\n",
      " [-0.   -0.05 -0.11 -0.1   0.04  0.08  0.06  0.04  0.01 -0.17]\n",
      " [-0.31 -0.06 -0.18 -0.29 -0.02  0.04  0.02 -0.    0.   -0.01]\n",
      " [-0.03 -0.04 -0.08 -0.13 -0.01  0.   -0.03 -0.05 -0.01 -0.07]\n",
      " [-0.01 -0.22 -0.05 -0.08 -0.06 -0.19 -0.16 -0.07 -0.05 -0.3 ]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.nan,precision=2)\n",
    "\n",
    "def feature_averages(trajectory,gamma=0.99):\n",
    "    horizon = len(trajectory)\n",
    "    return np.sum(np.multiply(trajectory,np.array([gamma**j for j in range(horizon)]).reshape(horizon,1)),axis=0)\n",
    "\n",
    "class RelEntIRL:\n",
    "    def __init__(self,expert_demos,nonoptimal_demos):\n",
    "        self.expert_demos = expert_demos\n",
    "        self.nonoptimal_demos = nonoptimal_demos\n",
    "        self.num_features = len(self.expert_demos[0][0])\n",
    "        self.weights = np.zeros((self.num_features,))\n",
    "        \n",
    "    def calculate_objective(self):\n",
    "        '''For the partition function Z($\\theta$), we just sum over all the exponents of their rewards, similar to\n",
    "        the equation above equation (6) in the original paper.'''\n",
    "        objective = np.dot(self.expert_feature,self.weights)\n",
    "        for i in range(self.nonoptimal_demos.shape[0]):\n",
    "            objective -= np.exp(np.dot(self.policy_features[i],self.weights))\n",
    "        return objective\n",
    "    \n",
    "    def calculate_expert_feature(self):\n",
    "        self.expert_feature = np.zeros_like(self.weights)\n",
    "        for i in range(len(self.expert_demos)):\n",
    "            self.expert_feature += feature_averages(self.expert_demos[i])\n",
    "        self.expert_feature /= len(self.expert_demos)\n",
    "        return self.expert_feature\n",
    "    \n",
    "    def train(self,step_size=1e-4,num_iters=50000,print_every=5000):\n",
    "        self.calculate_expert_feature()\n",
    "        self.policy_features = np.zeros((len(self.nonoptimal_demos),self.num_features))\n",
    "        for i in range(len(self.nonoptimal_demos)):\n",
    "            self.policy_features[i] = feature_averages(self.nonoptimal_demos[i])\n",
    "            \n",
    "        importance_sampling = np.zeros((len(self.nonoptimal_demos),))\n",
    "        for i in range(num_iters):\n",
    "            update = np.zeros_like(self.weights)\n",
    "            for j in range(len(self.nonoptimal_demos)):\n",
    "                importance_sampling[j] = np.exp(np.dot(self.policy_features[j],self.weights))\n",
    "            importance_sampling /= np.sum(importance_sampling,axis=0)\n",
    "            weighted_sum = np.sum(np.multiply(np.array([importance_sampling,]*self.policy_features.shape[1]).T,\\\n",
    "                                              self.policy_features),axis=0)\n",
    "            self.weights += step_size*(self.expert_feature - weighted_sum)\n",
    "            # One weird trick to ensure that the weights don't blow up.\n",
    "            self.weights = self.weights/np.linalg.norm(self.weights,keepdims=True)\n",
    "            if i%print_every == 0:\n",
    "                print \"Value of objective is: \" + str(self.calculate_objective())\n",
    "        \n",
    "    \n",
    "expert_trajectories = gw.generate_trajectory(optimal_policy)\n",
    "nonoptimal_trajectories = gw.generate_trajectory()\n",
    "relent = RelEntIRL(expert_trajectories,nonoptimal_trajectories)\n",
    "relent.train()\n",
    "print relent.weights.reshape(10,10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
