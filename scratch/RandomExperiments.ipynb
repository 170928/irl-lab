{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "<ul>\n",
    "<li> Currently, we are dealing with only deterministic policies. Have to extend the implementation to stochastic policies. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self,GRID_SIZE=50):\n",
    "        self.grid_size = GRID_SIZE\n",
    "        self.num_states = self.grid_size**2\n",
    "        self.rewards = np.random.choice([0,-1],p=[0.67,0.33],size=(self.grid_size,self.grid_size))\n",
    "        self.rewards[0,self.grid_size-1] = 1\n",
    "        self.goal_state = self.grid_size-1\n",
    "        self.actions = np.array([\"up\",\"down\",\"left\",\"right\"])\n",
    "        \n",
    "    def get_feature(self,state):\n",
    "        _ = [0]*self.num_states\n",
    "        _[state] = 1\n",
    "        return _\n",
    "        \n",
    "    \n",
    "    def get_rewards(self):\n",
    "        return self.rewards.flatten()\n",
    "    \n",
    "    def result_of_action(self,state,action):\n",
    "        state_coords = (state/self.grid_size,state%self.grid_size)\n",
    "        next_states = [(max(0,state_coords[0]-1),state_coords[1]),(min(self.grid_size-1,state_coords[0]+1),state_coords[1]),\\\n",
    "                      (state_coords[0],max(0,state_coords[1]-1)),(state_coords[0],min(self.grid_size-1,state_coords[1]+1))]\n",
    "        transition_probs = 0.1*np.ones((len(self.actions)))\n",
    "        transition_probs[np.where(self.actions == action)[0][0]] = 0.7\n",
    "        next_state = next_states[np.random.choice(range(len(next_states)),p=transition_probs)]\n",
    "        return next_state[0]*self.grid_size+next_state[1]\n",
    "    \n",
    "    def generate_trajectory(self,policy,num_trajectories=10):\n",
    "        trajectories = []\n",
    "        for i in range(num_trajectories):\n",
    "            trajectory = []\n",
    "            current_state = np.random.randint(self.num_states)\n",
    "            while current_state != self.goal_state and len(trajectory) < self.grid_size*3: # naive check, but will do\n",
    "                trajectory.append(self.get_feature(current_state))\n",
    "                current_state = self.result_of_action(current_state,policy[current_state])\n",
    "            trajectory.append(self.get_feature(self.goal_state))\n",
    "            trajectories.append(trajectory)\n",
    "        return np.array(trajectories)\n",
    "        \n",
    "    \n",
    "    def get_transition_probabilities(self,state,action):\n",
    "        '''While calculating the transition probabilities, we make the assumption that if you were in a cell along\n",
    "        the border, and you tried to make a transition outside the border with probability p, you end up not\n",
    "        moving with the same probability p.'''\n",
    "        transition_probs = np.zeros((self.grid_size,self.grid_size))\n",
    "        state_coords = (state/self.grid_size,state%self.grid_size)\n",
    "        if action == \"up\":\n",
    "            transition_probs[max(0,state_coords[0]-1),state_coords[1]] += 0.7 # up\n",
    "            transition_probs[state_coords[0],max(0,state_coords[1]-1)] += 0.1 # left\n",
    "            transition_probs[min(self.grid_size-1,state_coords[0]+1),state_coords[1]] += 0.1 # down\n",
    "            transition_probs[state_coords[0],min(self.grid_size-1,state_coords[1]+1)] += 0.1 #right\n",
    "        elif action == \"down\":\n",
    "            transition_probs[max(0,state_coords[0]-1),state_coords[1]] += 0.1\n",
    "            transition_probs[state_coords[0],max(0,state_coords[1]-1)] += 0.1\n",
    "            transition_probs[min(self.grid_size-1,state_coords[0]+1),state_coords[1]] += 0.7\n",
    "            transition_probs[state_coords[0],min(self.grid_size-1,state_coords[1]+1)] += 0.1\n",
    "        elif action == \"left\":\n",
    "            transition_probs[max(0,state_coords[0]-1),state_coords[1]] += 0.1\n",
    "            transition_probs[state_coords[0],max(0,state_coords[1]-1)] += 0.7\n",
    "            transition_probs[min(self.grid_size-1,state_coords[0]+1),state_coords[1]] += 0.1\n",
    "            transition_probs[state_coords[0],min(self.grid_size-1,state_coords[1]+1)] += 0.1\n",
    "        elif action == \"right\":\n",
    "            transition_probs[max(0,state_coords[0]-1),state_coords[1]] += 0.1\n",
    "            transition_probs[state_coords[0],max(0,state_coords[1]-1)] += 0.1\n",
    "            transition_probs[min(self.grid_size-1,state_coords[0]+1),state_coords[1]] += 0.1\n",
    "            transition_probs[state_coords[0],min(self.grid_size-1,state_coords[1]+1)] += 0.7\n",
    "        return transition_probs.flatten()\n",
    "    \n",
    "    def take_greedy_action(self,values):\n",
    "        values = values.reshape(self.grid_size,self.grid_size)\n",
    "        policy = np.repeat(\"random\",self.num_states)\n",
    "        for i in range(self.num_states):\n",
    "            state_coords = (i/self.grid_size,i%self.grid_size)\n",
    "            policy[i] = self.actions[np.argmax([values[max(0,state_coords[0]-1),state_coords[1]],\n",
    "                                                values[min(self.grid_size-1,state_coords[0]+1),state_coords[1]],\n",
    "                                                values[state_coords[0],max(0,state_coords[1]-1)],\n",
    "                                                values[state_coords[0],min(self.grid_size-1,state_coords[1]+1)]])]\n",
    "        return policy\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0 -1  0  1]\n",
      " [ 0 -1  0  0  0]\n",
      " [ 0 -1 -1  0 -1]\n",
      " [ 0  0  0 -1 -1]\n",
      " [-1 -1  0  0 -1]]\n"
     ]
    }
   ],
   "source": [
    "class PolicyIteration:\n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        self.values = np.zeros((self.env.num_states,))\n",
    "        self.policy = np.random.choice(self.env.actions,size=(self.env.num_states))\n",
    "    \n",
    "    def policy_evaluation(self,num_iters=10,gamma=0.99):\n",
    "        for i in range(num_iters):\n",
    "            transition_probs = np.zeros((self.env.num_states,self.env.num_states))\n",
    "            for j in range(self.env.num_states):\n",
    "                transition_probs[j] = self.env.get_transition_probabilities(j,self.policy[j])\n",
    "            self.values = self.env.get_rewards() + gamma*np.dot(transition_probs,self.values)\n",
    "    \n",
    "    def policy_iteration(self,num_iters=10):\n",
    "        for i in range(num_iters):\n",
    "            self.policy_evaluation()\n",
    "            self.policy = self.env.take_greedy_action(self.values)\n",
    "        return self.policy\n",
    "        \n",
    "\n",
    "gw = GridWorld(5)\n",
    "print gw.rewards\n",
    "pi = PolicyIteration(gw)\n",
    "optimal_policy = pi.policy_iteration(100)\n",
    "trajectories = gw.generate_trajectory(optimal_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
